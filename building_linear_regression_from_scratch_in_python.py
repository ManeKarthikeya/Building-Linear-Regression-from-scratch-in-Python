# -*- coding: utf-8 -*-
"""Building Linear Regression from scratch in Python.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1_Q16P1Gx3fiYZAdCqdMFdfj2iXLudjBT

**Linear Regression:**

**Y = wX + b**

Y --> Dependent Variable

X --> Indipendent Variable

w --> weight

b --> bias

**Gradient Descent:**

Gradient Descent is an optimazation algorithm used for minimizing the loss function in various machine leatning algorithms. It is used for updating the parameters of the learning model.

**w = w - a*dw**

**b = b - a*db**

**Learning Rate:**

Learning rate is a tuning parameter in an optimization algorithm that determines the step size at each iteration while moving toward a minimum of a loss function.

$\hat{y}^{(i)} = wx^{(i)} + b$

$dw = \frac{1}{m} \sum_{i=1}^{m} (\hat{y}^{(i)} - y^{(i)}) x^{(i)}$

$db = \frac{1}{m} \sum_{i=1}^{m} (\hat{y}^{(i)} - y^{(i)})$

$w := w - \alpha \, dw$

$b := b - \alpha \, db$
"""

# Importing numpy library
import numpy as np

"""**Linear Regression**"""

import numpy as np

class Linear_Regression():

    # Initializing parameters
    def __init__(self, learning_rate, no_of_iterations):
        self.learning_rate = learning_rate
        self.no_of_iterations = no_of_iterations

    # Training function
    def fit(self, X, Y):
        self.m, self.n = X.shape        # rows, columns
        self.w = np.zeros(self.n)       # weights
        self.b = 0                      # bias
        self.X = X
        self.Y = Y

        for i in range(self.no_of_iterations):
            self.update_weights()

    # Gradient descent weight update
    def update_weights(self):
        Y_prediction = self.predict(self.X)

        dw = -(2 * (self.X.T).dot(self.Y - Y_prediction)) / self.m
        db = -(2 * np.sum(self.Y - Y_prediction)) / self.m

        # update parameters
        self.w = self.w - self.learning_rate * dw
        self.b = self.b - self.learning_rate * db

    # Prediction
    def predict(self, X):
        return X.dot(self.w) + self.b

"""Using Linear Regression model for Prediction"""

# Importing the dependiencies
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split

"""Data Pre-Precossing"""

# loding the data from csv file to a pandas dataframe
salary_data = pd.read_csv('/content/salary_data.csv')

# Printing the first 5 columns of the dataframe
salary_data.head()

salary_data.tail()

# number of rows & columns in the dataframe
salary_data.shape

# checking for the missing values
salary_data.isnull().sum()

"""Splitting the features & target"""

X = salary_data.iloc[:, :-1].values
Y = salary_data.iloc[:,1].values

print(X)

print(Y)

"""Splitting the dataset into training & test data"""

X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size =0.33, random_state=2)

"""Training the Linear Regression model"""

model = Linear_Regression(learning_rate=0.02, no_of_iterations=1000)

model.fit(X_train, Y_train)

# Printing the Parameter values ( weights & bidas )
print('weight = ' , model.w[0])
print('bias = ' , model.b)

"""y = 9514(X) + 23697


salary = 9514(experience) + 23697

Predict the salary value for the test data
"""

test_data_prediction = model.predict(X_test)

print(test_data_prediction)

"""visualizing the predicted values & actual values"""

plt.scatter(X_test, Y_test, color = 'red')
plt.plot(X_test, test_data_prediction, color = 'blue')
plt.xlabel('Work Experience')
plt.ylabel('Salary')
plt.title('Salary vs Experience')
plt.show()

"""Preduction of Salary ( where input is years of experience )"""

years_of_experience = float(input("Please enter the years of experience: "))
print(f"Years of Experience entered: {years_of_experience}")

input_experience = np.array([[years_of_experience]])
predicted_salary = model.predict(input_experience)
print(f"Predicted Salary for {years_of_experience} years of experience: {predicted_salary[0]:.2f}")